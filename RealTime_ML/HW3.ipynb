{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HW3.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FQJFEL6RZTUa"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import torch.nn as nn\n",
        "from matplotlib import pyplot as plt\n",
        "torch.cuda.is_available()\n",
        "import datetime\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "#Daniel Sullivan\n",
        "#801095863"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import datasets\n",
        "data_path = 'drive/MyDrive/Colab Notebooks/'\n",
        "cifar10 = datasets.CIFAR10(data_path, train=True, download=True, transform = transforms.ToTensor())\n",
        "cifar10_val = datasets.CIFAR10(data_path, train=False, download=True, transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.4195, 0.4823, 0.4468), (0.2470, 0.2435, 0.2616))]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TnScGuO_Z3Eg",
        "outputId": "eb67c34d-c853-468d-f346-11d85d8b67c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Problem 1"
      ],
      "metadata": {
        "id": "qv2urYnbq_Jg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(cifar10, batch_size=64, shuffle=True)\n",
        "\n",
        "class Net(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)\n",
        "    self.conv2 = nn.Conv2d(16, 16, kernel_size=3, padding=1) # Part 2\n",
        "    self.conv3 = nn.Conv2d(16, 8, kernel_size=3, padding=1)  \n",
        "    self.fc1 = nn.Linear(4*4*8, 32)\n",
        "    self.fc2 = nn.Linear(32, 10)\n",
        "\n",
        "  def forward(self, x):\n",
        "    out = F.max_pool2d(torch.tanh(self.conv1(x)), 2)\n",
        "    out = F.max_pool2d(torch.tanh(self.conv2(out)), 2) # Part 2\n",
        "    out = F.max_pool2d(torch.tanh(self.conv3(out)), 2)\n",
        "    out = out.view(-1, 4*4*8)\n",
        "    out = torch.tanh(self.fc1(out))\n",
        "    out = self.fc2(out)\n",
        "    return out"
      ],
      "metadata": {
        "id": "typyG0L4af2d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def training_loop(n_epochs, optimizer, model, loss_fn, train_loader):\n",
        "  for epoch in range(1, n_epochs + 1):\n",
        "    loss_train = 0.0\n",
        "    for imgs, labels in train_loader:\n",
        "      imgs = imgs.to(device='cuda')\n",
        "      labels = labels.to(device='cuda')\n",
        "      outputs = model(imgs)\n",
        "\n",
        "      loss = loss_fn(outputs, labels)\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      loss.backward()\n",
        "\n",
        "      optimizer.step()\n",
        "\n",
        "      loss_train += loss.item()\n",
        "\n",
        "    if epoch == 1 or epoch % 10 == 0:\n",
        "      print('{} Epoch {}, Training Loss {}'.format(datetime.datetime.now(), epoch, loss_train / len(train_loader)))"
      ],
      "metadata": {
        "id": "t2tisfRAdXlo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Net().to(device = 'cuda')\n",
        "optimizer = optim.SGD(model.parameters(), lr = 1e-2)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "training_loop(300, optimizer, model, loss_fn, train_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3HlUXkuvewao",
        "outputId": "64613af4-740b-4ea9-d304-52accddcc384"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-03-23 14:29:42.579464 Epoch 1, Training Loss 2.3003357145792385\n",
            "2022-03-23 14:31:41.120742 Epoch 10, Training Loss 1.546201981851817\n",
            "2022-03-23 14:33:52.886411 Epoch 20, Training Loss 1.2697578250142314\n",
            "2022-03-23 14:36:04.006029 Epoch 30, Training Loss 1.121033842834975\n",
            "2022-03-23 14:38:14.051685 Epoch 40, Training Loss 1.0273683696146816\n",
            "2022-03-23 14:40:24.591284 Epoch 50, Training Loss 0.9641874795374663\n",
            "2022-03-23 14:42:34.812360 Epoch 60, Training Loss 0.9190347962977027\n",
            "2022-03-23 14:44:44.140261 Epoch 70, Training Loss 0.884326749521753\n",
            "2022-03-23 14:46:53.177298 Epoch 80, Training Loss 0.8577099882275857\n",
            "2022-03-23 14:49:04.051202 Epoch 90, Training Loss 0.8340919273512443\n",
            "2022-03-23 14:51:13.336031 Epoch 100, Training Loss 0.8163158984669029\n",
            "2022-03-23 14:53:22.154276 Epoch 110, Training Loss 0.7999661336926853\n",
            "2022-03-23 14:55:31.106005 Epoch 120, Training Loss 0.7862141566050936\n",
            "2022-03-23 14:57:40.678758 Epoch 130, Training Loss 0.7723664066294575\n",
            "2022-03-23 14:59:50.459839 Epoch 140, Training Loss 0.7630683019795381\n",
            "2022-03-23 15:01:59.547558 Epoch 150, Training Loss 0.750516315098004\n",
            "2022-03-23 15:04:08.666893 Epoch 160, Training Loss 0.7423116018247726\n",
            "2022-03-23 15:06:17.780941 Epoch 170, Training Loss 0.7353151624312486\n",
            "2022-03-23 15:08:26.641894 Epoch 180, Training Loss 0.7252049976983643\n",
            "2022-03-23 15:10:35.245160 Epoch 190, Training Loss 0.7197632395931522\n",
            "2022-03-23 15:12:43.587907 Epoch 200, Training Loss 0.7144736171607167\n",
            "2022-03-23 15:14:52.281347 Epoch 210, Training Loss 0.7072480711943049\n",
            "2022-03-23 15:17:00.711797 Epoch 220, Training Loss 0.7028582710629839\n",
            "2022-03-23 15:19:11.080138 Epoch 230, Training Loss 0.6975068421772374\n",
            "2022-03-23 15:21:21.472578 Epoch 240, Training Loss 0.6934754867154314\n",
            "2022-03-23 15:23:33.566093 Epoch 250, Training Loss 0.6891523310747902\n",
            "2022-03-23 15:25:45.679347 Epoch 260, Training Loss 0.6850438948025179\n",
            "2022-03-23 15:27:57.669003 Epoch 270, Training Loss 0.6807013832013625\n",
            "2022-03-23 15:30:09.093130 Epoch 280, Training Loss 0.6778098962572224\n",
            "2022-03-23 15:32:19.709826 Epoch 290, Training Loss 0.6741421111404439\n",
            "2022-03-23 15:34:29.763163 Epoch 300, Training Loss 0.6717667746574373\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#train_loader = torch.utils.data.DataLoader(cifar10, batch_size=64, shuffle=False)\n",
        "\n",
        "#val_loader = torch.utils.data.DataLoader(cifar10_val, batch_size=64, shuffle=False)\n",
        "\n",
        "def validate(model, train_loader, val_loader):\n",
        "  for name, loader in [(\"train\", train_loader), (\"val\", val_loader)]:\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "      for imgs, labels in loader:\n",
        "        imgs = imgs.to(device='cuda')\n",
        "        labels = labels.to(device='cuda')\n",
        "        outputs = model(imgs)\n",
        "        _, predicted = torch.max(outputs, dim=1)\n",
        "        total += labels.shape[0]\n",
        "        correct += int((predicted == labels).sum())\n",
        "\n",
        "    print(\"Accuracy {}: {:.2f}\".format(name, correct / total))\n",
        "\n",
        "#validate(model, train_loader, val_loader)"
      ],
      "metadata": {
        "id": "kdk0RsnDfmCX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Problem 2"
      ],
      "metadata": {
        "id": "2ZSGBH9OhXbi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ResBlock(nn.Module):\n",
        "  def __init__(self, n_chans):\n",
        "    super(ResBlock, self).__init__()\n",
        "    self.conv = nn.Conv2d(n_chans, n_chans, kernel_size = 3, padding = 1, bias = False)\n",
        "    self.batch_norm = nn.BatchNorm2d(num_features=n_chans)\n",
        "    torch.nn.init.kaiming_normal_(self.conv.weight, nonlinearity='relu')\n",
        "    torch.nn.init.constant_(self.batch_norm.weight, 0.5)\n",
        "    torch.nn.init.zeros_(self.batch_norm.bias)\n",
        "\n",
        "  def forward(self, x):\n",
        "    out = self.conv(x)\n",
        "    out = self.batch_norm(out)\n",
        "    out = torch.relu(out)\n",
        "    return out + x"
      ],
      "metadata": {
        "id": "y-eUsuXYrC7Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ResNet_10(nn.Module):\n",
        "  def __init__(self, n_chans1 = 32, n_blocks=10):\n",
        "    super().__init__()\n",
        "    self.n_chans1 = n_chans1\n",
        "    self.conv1 = nn.Conv2d(3, n_chans1, kernel_size = 3, padding = 1)\n",
        "    self.resblocks = nn.Sequential(*(n_blocks * [ResBlock(n_chans = n_chans1)]))\n",
        "    self.fc1 = nn.Linear(8 *8 * n_chans1, 32)\n",
        "    self.fc2 = nn.Linear(32, 10)\n",
        "\n",
        "  def forward(self, x):\n",
        "    out = F.max_pool2d(torch.relu(self.conv1(x)), 2)\n",
        "    out = self.resblocks(out)\n",
        "    out = F.max_pool2d(out, 2)\n",
        "    out = out.view(-1, 8 * 8 * self.n_chans1)\n",
        "    out = torch.relu(self.fc1(out))\n",
        "    out = self.fc2(out)\n",
        "    return out"
      ],
      "metadata": {
        "id": "hfvY5ap3stYF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = torch.utils.data.DataLoader(cifar10, batch_size=64, shuffle=True)\n",
        "\n",
        "model2 = ResNet_10().to(device = 'cuda')\n",
        "optimizer = optim.SGD(model2.parameters(), lr = 3e-3)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "training_loop(300, optimizer, model2, loss_fn, train_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jFAu52uTuCCg",
        "outputId": "ecac97a9-7eea-43f1-f49c-8d47b3df7e8f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-03-23 15:35:11.071012 Epoch 1, Training Loss 1.805843689709978\n",
            "2022-03-23 15:39:23.620004 Epoch 10, Training Loss 0.9612440843411418\n",
            "2022-03-23 15:44:05.600064 Epoch 20, Training Loss 0.7624676719193568\n",
            "2022-03-23 15:48:46.615787 Epoch 30, Training Loss 0.6327849361292847\n",
            "2022-03-23 15:53:28.817180 Epoch 40, Training Loss 0.5267089729761834\n",
            "2022-03-23 15:58:08.536401 Epoch 50, Training Loss 0.44487251598588035\n",
            "2022-03-23 16:02:49.515674 Epoch 60, Training Loss 0.3750768093882924\n",
            "2022-03-23 16:07:30.206053 Epoch 70, Training Loss 0.3109951575698755\n",
            "2022-03-23 16:12:10.779778 Epoch 80, Training Loss 0.25670885155572915\n",
            "2022-03-23 16:16:52.540677 Epoch 90, Training Loss 0.21688751167501025\n",
            "2022-03-23 16:21:34.996040 Epoch 100, Training Loss 0.18954423415329297\n",
            "2022-03-23 16:26:15.340669 Epoch 110, Training Loss 0.14127037594394992\n",
            "2022-03-23 16:30:59.755591 Epoch 120, Training Loss 0.14085899077384445\n",
            "2022-03-23 16:35:43.396529 Epoch 130, Training Loss 0.09663171721074511\n",
            "2022-03-23 16:40:24.027923 Epoch 140, Training Loss 0.11147790731233366\n",
            "2022-03-23 16:45:02.370157 Epoch 150, Training Loss 0.06322165884439597\n",
            "2022-03-23 16:49:43.218102 Epoch 160, Training Loss 0.06956033143357795\n",
            "2022-03-23 16:54:24.175572 Epoch 170, Training Loss 0.09300496690558827\n",
            "2022-03-23 16:59:03.961702 Epoch 180, Training Loss 0.1355623132841009\n",
            "2022-03-23 17:03:42.965900 Epoch 190, Training Loss 0.02848400700586679\n",
            "2022-03-23 17:08:22.917232 Epoch 200, Training Loss 0.05053731804276528\n",
            "2022-03-23 17:13:00.919466 Epoch 210, Training Loss 0.015519140561482252\n",
            "2022-03-23 17:17:40.217962 Epoch 220, Training Loss 0.13039764820519464\n",
            "2022-03-23 17:22:20.818516 Epoch 230, Training Loss 0.010668539570149063\n",
            "2022-03-23 17:26:59.216488 Epoch 240, Training Loss 0.008094570767792427\n",
            "2022-03-23 17:31:37.735631 Epoch 250, Training Loss 0.014233801936957917\n",
            "2022-03-23 17:36:16.540110 Epoch 260, Training Loss 0.07244869939925726\n",
            "2022-03-23 17:40:57.669232 Epoch 270, Training Loss 0.007287913754232861\n",
            "2022-03-23 17:45:38.009510 Epoch 280, Training Loss 0.004650972678799175\n",
            "2022-03-23 17:50:20.476273 Epoch 290, Training Loss 0.0046823479082879595\n",
            "2022-03-23 17:55:00.635039 Epoch 300, Training Loss 0.0038636588383048577\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = torch.utils.data.DataLoader(cifar10, batch_size=64, shuffle=False)\n",
        "\n",
        "val_loader = torch.utils.data.DataLoader(cifar10_val, batch_size=64, shuffle=False)\n",
        "\n",
        "validate(model2, train_loader, val_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vxDLAs1TvN5Y",
        "outputId": "8bcdf91a-c8a7-4687-b1f3-7ddc1a3ea2bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy train: 0.88\n",
            "Accuracy val: 0.41\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ResBlock_Drop(nn.Module):\n",
        "  def __init__(self, n_chans):\n",
        "    super(ResBlock_Drop, self).__init__()\n",
        "    self.conv = nn.Conv2d(n_chans, n_chans, kernel_size = 3, padding = 1, bias = False)\n",
        "    self.conv_dropout = nn.Dropout2d(p = 0.3)\n",
        "\n",
        "  def forward(self, x):\n",
        "    out = self.conv(x)\n",
        "    out = torch.relu(out)\n",
        "    out = out + x\n",
        "    out = self.conv_dropout(out)\n",
        "    return out"
      ],
      "metadata": {
        "id": "Bvdz1R4-NOT9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ResNet_10_Drop(nn.Module):\n",
        "  def __init__(self, n_chans1 = 32, n_blocks=10):\n",
        "    super().__init__()\n",
        "    self.n_chans1 = n_chans1\n",
        "    self.conv1 = nn.Conv2d(3, n_chans1, kernel_size = 3, padding = 1)\n",
        "    self.resblocks = nn.Sequential(*(n_blocks * [ResBlock_Drop(n_chans = n_chans1)]))\n",
        "    self.fc1 = nn.Linear(8 *8 * n_chans1, 32)\n",
        "    self.fc2 = nn.Linear(32, 10)\n",
        "\n",
        "  def forward(self, x):\n",
        "    out = F.max_pool2d(torch.relu(self.conv1(x)), 2)\n",
        "    out = self.resblocks(out)\n",
        "    out = F.max_pool2d(out, 2)\n",
        "    out = out.view(-1, 8 * 8 * self.n_chans1)\n",
        "    out = torch.relu(self.fc1(out))\n",
        "    out = self.fc2(out)\n",
        "    return out"
      ],
      "metadata": {
        "id": "l2f99wgGuUnG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = torch.utils.data.DataLoader(cifar10, batch_size=64, shuffle=True)\n",
        "\n",
        "model3 = ResNet_10_Drop().to(device = 'cuda')\n",
        "optimizer = optim.SGD(model3.parameters(), lr = 3e-3)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "training_loop(300, optimizer, model3, loss_fn, train_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ytR6dJWvgMl",
        "outputId": "8dd6f199-6217-4652-ac36-d53c076de653"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-03-27 23:44:32.632116 Epoch 1, Training Loss 2.2939971868339404\n",
            "2022-03-27 23:48:46.166564 Epoch 10, Training Loss 1.6863128201431021\n",
            "2022-03-27 23:53:26.862375 Epoch 20, Training Loss 1.4977321261944978\n",
            "2022-03-27 23:58:07.163457 Epoch 30, Training Loss 1.3949790778367415\n",
            "2022-03-28 00:02:47.252744 Epoch 40, Training Loss 1.3307604270670421\n",
            "2022-03-28 00:07:27.637759 Epoch 50, Training Loss 1.2760224964307703\n",
            "2022-03-28 00:12:08.062812 Epoch 60, Training Loss 1.2474415838870856\n",
            "2022-03-28 00:16:48.222933 Epoch 70, Training Loss 1.2208610536039943\n",
            "2022-03-28 00:21:28.336857 Epoch 80, Training Loss 1.197258521223922\n",
            "2022-03-28 00:26:08.308963 Epoch 90, Training Loss 1.1754911881121224\n",
            "2022-03-28 00:30:48.122508 Epoch 100, Training Loss 1.1532476008548151\n",
            "2022-03-28 00:35:27.960905 Epoch 110, Training Loss 1.136587511547996\n",
            "2022-03-28 00:40:07.719954 Epoch 120, Training Loss 1.1210269153575458\n",
            "2022-03-28 00:44:47.882089 Epoch 130, Training Loss 1.113270875438095\n",
            "2022-03-28 00:49:28.078713 Epoch 140, Training Loss 1.1008127791344966\n",
            "2022-03-28 00:54:08.788028 Epoch 150, Training Loss 1.0883921203405962\n",
            "2022-03-28 00:58:53.242897 Epoch 160, Training Loss 1.08110507263247\n",
            "2022-03-28 01:03:35.459966 Epoch 170, Training Loss 1.0737985184278025\n",
            "2022-03-28 01:08:18.910304 Epoch 180, Training Loss 1.0600471030110898\n",
            "2022-03-28 01:12:59.078273 Epoch 190, Training Loss 1.0621762735307063\n",
            "2022-03-28 01:17:39.550704 Epoch 200, Training Loss 1.0506408865494496\n",
            "2022-03-28 01:22:19.206899 Epoch 210, Training Loss 1.047442643310103\n",
            "2022-03-28 01:26:57.916334 Epoch 220, Training Loss 1.044189847262619\n",
            "2022-03-28 01:31:35.983129 Epoch 230, Training Loss 1.0410696889280968\n",
            "2022-03-28 01:36:16.585881 Epoch 240, Training Loss 1.0341811366093434\n",
            "2022-03-28 01:40:58.962807 Epoch 250, Training Loss 1.0307304936906565\n",
            "2022-03-28 01:45:40.712344 Epoch 260, Training Loss 1.02296116086833\n",
            "2022-03-28 01:50:22.450109 Epoch 270, Training Loss 1.0200134488322852\n",
            "2022-03-28 01:55:05.608688 Epoch 280, Training Loss 1.0104108386484862\n",
            "2022-03-28 01:59:48.183811 Epoch 290, Training Loss 1.0107343620656397\n",
            "2022-03-28 02:04:30.035099 Epoch 300, Training Loss 1.0049358998113276\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = torch.utils.data.DataLoader(cifar10, batch_size=64, shuffle=False)\n",
        "\n",
        "val_loader = torch.utils.data.DataLoader(cifar10_val, batch_size=64, shuffle=False)\n",
        "\n",
        "validate(model3, train_loader, val_loader)"
      ],
      "metadata": {
        "id": "7_KEUEKdQBM8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c260df02-7176-4ff2-f7a5-b6f584651ef9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy train: 0.65\n",
            "Accuracy val: 0.32\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ResBlock_Penalties(nn.Module):\n",
        "  def __init__(self, n_chans):\n",
        "    super(ResBlock_Penalties, self).__init__()\n",
        "    self.conv = nn.Conv2d(n_chans, n_chans, kernel_size = 3, padding = 1, bias = False)\n",
        "\n",
        "  def forward(self, x):\n",
        "    out = self.conv(x)\n",
        "    out = torch.relu(out)\n",
        "    return out + x"
      ],
      "metadata": {
        "id": "IRCws3WHQsMz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def training_loop_penalties(n_epochs, optimizer, model, loss_fn, train_loader):\n",
        "  for epoch in range(1, n_epochs + 1):\n",
        "    loss_train = 0.0\n",
        "    for imgs, labels in train_loader:\n",
        "      imgs = imgs.to(device='cuda')\n",
        "      labels = labels.to(device='cuda')\n",
        "      outputs = model(imgs)\n",
        "      loss = loss_fn(outputs, labels)\n",
        "\n",
        "      l2_lambda = 0.001\n",
        "      l2_norm = sum(p.pow(2.0).sum() for p in model.parameters())\n",
        "      loss = loss + l2_lambda * l2_norm\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      loss.backward()\n",
        "\n",
        "      optimizer.step()\n",
        "\n",
        "      loss_train += loss.item()\n",
        "\n",
        "    if epoch == 1 or epoch % 10 == 0:\n",
        "      print('{} Epoch {}, Training Loss {}'.format(datetime.datetime.now(), epoch, loss_train / len(train_loader)))"
      ],
      "metadata": {
        "id": "9zsLjrKaRPZr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ResNet_10_Penalties(nn.Module):\n",
        "  def __init__(self, n_chans1 = 32, n_blocks=10):\n",
        "    super().__init__()\n",
        "    self.n_chans1 = n_chans1\n",
        "    self.conv1 = nn.Conv2d(3, n_chans1, kernel_size = 3, padding = 1)\n",
        "    self.resblocks = nn.Sequential(*(n_blocks * [ResBlock_Penalties(n_chans = n_chans1)]))\n",
        "    self.fc1 = nn.Linear(8 *8 * n_chans1, 32)\n",
        "    self.fc2 = nn.Linear(32, 10)\n",
        "\n",
        "  def forward(self, x):\n",
        "    out = F.max_pool2d(torch.relu(self.conv1(x)), 2)\n",
        "    out = self.resblocks(out)\n",
        "    out = F.max_pool2d(out, 2)\n",
        "    out = out.view(-1, 8 * 8 * self.n_chans1)\n",
        "    out = torch.relu(self.fc1(out))\n",
        "    out = self.fc2(out)\n",
        "    return out"
      ],
      "metadata": {
        "id": "EFdLhnziR9D7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = torch.utils.data.DataLoader(cifar10, batch_size=64, shuffle=True)\n",
        "\n",
        "model4 = ResNet_10_Penalties().to(device = 'cuda')\n",
        "optimizer = optim.SGD(model4.parameters(), lr = 3e-3)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "training_loop_penalties(300, optimizer, model4, loss_fn, train_loader)"
      ],
      "metadata": {
        "id": "LIHKFj60SLCN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2906956e-976b-4bf4-bf7e-d8702395d799"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-03-28 02:05:22.538298 Epoch 1, Training Loss 1.947956172706526\n",
            "2022-03-28 02:09:54.502553 Epoch 10, Training Loss 1.037286742416489\n",
            "2022-03-28 02:14:50.755368 Epoch 20, Training Loss 0.8038683915534592\n",
            "2022-03-28 02:19:42.497645 Epoch 30, Training Loss 0.6535343272835398\n",
            "2022-03-28 02:24:27.196270 Epoch 40, Training Loss 0.542217904535096\n",
            "2022-03-28 02:29:11.824325 Epoch 50, Training Loss 0.4651967398155376\n",
            "2022-03-28 02:33:56.132067 Epoch 60, Training Loss 0.40900400587741065\n",
            "2022-03-28 02:38:41.090691 Epoch 70, Training Loss 0.3691396387222478\n",
            "2022-03-28 02:43:26.815051 Epoch 80, Training Loss 0.34137875438117615\n",
            "2022-03-28 02:48:12.122658 Epoch 90, Training Loss 0.32672781695414077\n",
            "2022-03-28 02:52:55.255196 Epoch 100, Training Loss 0.3161836064723142\n",
            "2022-03-28 02:57:37.539984 Epoch 110, Training Loss 0.3168284235631718\n",
            "2022-03-28 03:02:18.045648 Epoch 120, Training Loss 0.2864150684088697\n",
            "2022-03-28 03:06:58.954742 Epoch 130, Training Loss 0.2813523673188046\n",
            "2022-03-28 03:11:39.695834 Epoch 140, Training Loss 0.26822110907653407\n",
            "2022-03-28 03:16:19.969128 Epoch 150, Training Loss 0.2712447501509391\n",
            "2022-03-28 03:21:00.561335 Epoch 160, Training Loss 0.2606391615193823\n",
            "2022-03-28 03:25:41.352659 Epoch 170, Training Loss 0.2561057074676694\n",
            "2022-03-28 03:30:22.875122 Epoch 180, Training Loss 0.26919805584356304\n",
            "2022-03-28 03:35:03.598270 Epoch 190, Training Loss 0.25029987416913746\n",
            "2022-03-28 03:39:45.498384 Epoch 200, Training Loss 0.26518742580090643\n",
            "2022-03-28 03:44:27.571549 Epoch 210, Training Loss 0.24893745705675896\n",
            "2022-03-28 03:49:11.487690 Epoch 220, Training Loss 0.23774474767772744\n",
            "2022-03-28 03:53:54.284136 Epoch 230, Training Loss 0.24162524940488894\n",
            "2022-03-28 03:58:37.379286 Epoch 240, Training Loss 0.2274789558842664\n",
            "2022-03-28 04:03:18.397971 Epoch 250, Training Loss 0.2380800644683716\n",
            "2022-03-28 04:07:58.863498 Epoch 260, Training Loss 0.24007616876183874\n",
            "2022-03-28 04:12:40.838878 Epoch 270, Training Loss 0.23575372588070456\n",
            "2022-03-28 04:17:23.893674 Epoch 280, Training Loss 0.21613955364355344\n",
            "2022-03-28 04:22:07.323621 Epoch 290, Training Loss 0.24275467641975568\n",
            "2022-03-28 04:26:50.045914 Epoch 300, Training Loss 0.21776929364332456\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = torch.utils.data.DataLoader(cifar10, batch_size=64, shuffle=False)\n",
        "\n",
        "val_loader = torch.utils.data.DataLoader(cifar10_val, batch_size=64, shuffle=False)\n",
        "\n",
        "validate(model4, train_loader, val_loader)"
      ],
      "metadata": {
        "id": "ST4sdZ6ASRDF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a83d6dfd-a611-45b4-f114-e437da02198e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy train: 0.92\n",
            "Accuracy val: 0.43\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "jw10COsYzfBc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}